# NLP_and_Multimodal_Distillation

### NLP

#### Pre-training

(1) MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. \[[link](https://arxiv.org/abs/2002.10957.pdf)\]

(2) Contrastive Distillation on Intermediate Representations for Language Model Compression. \[[link](https://arxiv.org/abs/2009.14167.pdf)\]

(3) DisCo: Effective Knowledge Distillation For Contrastive Learning of Sentence Embeddings. \[[link](https://arxiv.org/abs/2112.05638.pdf)\]

#### Text Retrieval

(1) Adversarial Retriever-Ranker for dense text retrieval. \[[link](https://arxiv.org/abs/2110.03611.pdf)\]

(2) LED: Lexicon-Enlightened Dense Retriever for Large-Scale Retrieval. \[[link](https://arxiv.org/pdf/2208.13661.pdf)\]

(3) EMBEDDISTILL: A Geometric Knowledge Distillation For Information Retrieval. \[[link](https://openreview.net/pdf?id=-aEuKX6zQKmr)\]

#### Neural Ranking

(1) In defense of dual-encoders for neural ranking. \[[link](https://proceedings.mlr.press/v162/menon22a/menon22a.pdf)\]

### Multimodal Learning

(1) Masked Autoencoders Enable Efficient Knowledge Distillers. \[[link](https://arxiv.org/pdf/2208.12256.pdf)\]

(2) Relational Knowledge Distillation. \[[link](https://arxiv.org/abs/1904.05068.pdf)\]

(3) MiniVLM: A Smaller and Faster Vision-Language Model. \[[link](https://arxiv.org/abs/2012.06946.pdf)\]

(4) ADVL: Adaptive Distillation For Vision-Language Task. \[[link](https://openreview.net/pdf?id=8-2sjUPp_YD)\]

(5) The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation. \[[link](https://openreview.net/pdf?id=w0QXrZ3N-s)\]

(6) Compressing Visual-linguistic Model via Knowledge Distillation. \[[link](https://arxiv.org/abs/2104.02096.pdf)\]

(7) Distilled Dual-Encoder Model for Vision-Language Understanding. \[[link](https://arxiv.org/abs/2112.08723.pdf)\]

(8) XDBERT: Distilling Visual Information to BERT from Cross-Modahttps://arxiv.org/pdf/2203.00048.pdfl Systems to Improve Language Understanding. \[[link](https://aclanthology.org/2022.acl-short.52.pdf)\]

(9) Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks. \[[link](https://arxiv.org/pdf/2204.10496.pdf)\]

(10) Multi-modal Alignment using Representation Codebook. \[[link](https://arxiv.org/pdf/2203.00048.pdf)\]




